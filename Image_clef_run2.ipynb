{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0Nsc0y4nV2V"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/drive/MyDrive/Touche clef/trainingset-ideology-power.zip' '/content/'"
      ],
      "metadata": {
        "id": "lkF_iSQLpZuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/trainingset-ideology-power.zip'\n",
        "\n",
        "# Function to read TSV files from a specific folder within a zip file and return a DataFrame\n",
        "def read_tsv_from_zip_folder(zip_file_path, folder_name):\n",
        "    dataframes = []\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        for file_name in zip_ref.namelist():\n",
        "            if file_name.startswith(folder_name) and file_name.endswith('.tsv'):\n",
        "                with zip_ref.open(file_name) as file:\n",
        "                    # Read TSV file into DataFrame\n",
        "                    df = pd.read_csv(io.TextIOWrapper(file, 'utf-8'), sep='\\t')\n",
        "                    # Append DataFrame to the list\n",
        "                    dataframes.append(df)\n",
        "    # Concatenate all DataFrames into one\n",
        "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "# Specify the folder name you want to extract files from\n",
        "specific_folder_name = 'power'\n",
        "\n",
        "# Call the function to read TSV files from the specified folder within the zip file\n",
        "df = read_tsv_from_zip_folder(zip_file_path, specific_folder_name)\n",
        "\n",
        "# Now, df contains the combined DataFrame from all TSV files in the specified folder within the zip file\n"
      ],
      "metadata": {
        "id": "-QRCIwoBpcKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Load data  # Replace 'your_data.csv' with your data file path\n",
        "df=df.dropna()\n",
        "# Prepare labels\n",
        "train_labels = df['label']\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    df['text_en'], train_labels, test_size=0.20, random_state=42\n",
        ")\n",
        "train_data  = df['text_en']\n",
        "train_labels = df['label']\n",
        "# Replace NaN values in 'text_en' column with empty string in training and test data\n",
        "train_data = train_data.fillna('')\n",
        "val_data = val_data.fillna('')\n",
        "\n",
        "# Define TfidfVectorizer for text data\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=10000, ngram_range=(1, 2), min_df=2, max_df=0.9, stop_words='english'\n",
        ")\n",
        "\n",
        "# Define class weights based on training labels\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Create a pipeline with TfidfVectorizer and LogisticRegression model\n",
        "pipeline = make_pipeline(vectorizer, LogisticRegression(class_weight=class_weights_dict, random_state=42))\n",
        "\n",
        "# Fit the model on training data\n",
        "pipeline.fit(train_data, train_labels)\n",
        "\n",
        "# Predict on the validation set\n",
        "val_predictions = pipeline.predict(val_data)\n",
        "\n",
        "# Evaluate the model using F1 score\n",
        "val_f1 = f1_score(val_labels, val_predictions)\n",
        "print(f\"Validation F1 Score: {val_f1}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(val_labels, val_predictions))\n",
        "\n",
        "# Predict on the test data\n",
        "test_data = df['text_en']\n",
        "test_data = test_data.fillna('')\n",
        "\n",
        "test_predictions = pipeline.predict(test_data)\n",
        "\n",
        "# Create a DataFrame with the predictions\n",
        "output_df = pd.DataFrame({'id': df['id'], 'label': test_predictions})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_df.to_csv('predictions.csv', index=False)\n",
        "print(\"Predictions saved to predictions.csv\")\n"
      ],
      "metadata": {
        "id": "xLTrjLDzpgh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_f2 = f1_score(df['label'], test_predictions)\n",
        "print(val_f2)"
      ],
      "metadata": {
        "id": "CPeUFCZMpijo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(pipeline, 'power logistic f1-77.joblib')\n",
        "print(\"Model saved to best_model.joblib\")"
      ],
      "metadata": {
        "id": "SREpHuCTplcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path to the zip file\n",
        "zip_file_path = '/content/ideology-power-st-testset.zip'\n",
        "\n",
        "# Specify the directory where you want to extract the files\n",
        "extract_dir = 'content/'\n",
        "\n",
        "# Create a ZipFile object\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents of the zip file\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"Files extracted successfully to\", extract_dir)"
      ],
      "metadata": {
        "id": "sGszyrRrpqlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Path to the TSV file\n",
        "file_path = '/content/content/power/power-es-test.tsv'\n",
        "fpath='pixel-power-es-run1.tsv'\n",
        "\n",
        "# Read the TSV file into a DataFrame\n",
        "df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "GSMFobsypvFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib  # Import joblib for loading the model\n",
        "all=\"ua\"\n",
        "fpath=\"pixel-power-\"+all+\"-run2.tsv\"\n",
        "file_path = \"/content/content/power/power-\"+all+\"-test.tsv\"\n",
        "\n",
        "# Read the TSV file into a DataFrame\n",
        "df = pd.read_csv(file_path, sep='\\t')\n",
        "\n",
        "# Load the saved model using joblib # Replace 'best_model.joblib' with the path to your saved model file\n",
        "# Load test data\n",
        "test_data = df  # Replace 'your_test_data.csv' with your test data file path\n",
        "\n",
        "# Replace NaN values in 'text_en' column with empty string in test data\n",
        "test_data['text_en'] = test_data['text_en'].fillna('')\n",
        "\n",
        "# Predict on the test data using the loaded model\n",
        "test_predictions = model.predict(test_data['text_en'])\n",
        "\n",
        "# Create a DataFrame with the predictions\n",
        "output_df = pd.DataFrame({'id': test_data['id'], 'label': test_predictions})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_df.to_csv(fpath, sep='\\t', columns=['id', 'label'], header=False, index=False)\n",
        "print(\"Predictions saved to \",fpath)"
      ],
      "metadata": {
        "id": "AJu5TbHkp-SK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}